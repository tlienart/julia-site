<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv=x-ua-compatible  content="ie=edge"> <meta http-equiv=content-type  content="text/html; charset=utf-8" /> <meta name=author  content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." /> <meta name=description  content="The official website for the Julia Language. Julia is a language that is fast, dynamic, easy to use, and open source. Click here to learn more."/> <meta property="og:title" content="The Julia Language"/> <meta property="og:image" content="http://www.julialang.org/images/julia-open-graph.png"/> <meta property="og:description" content="Official website for the Julia programming language"/> <link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel=stylesheet > <link rel=stylesheet  href="/julia-site/libs/bootstrap/bootstrap.min.css" /> <link rel=stylesheet  href="/julia-site/css/app.css" /> <link rel=stylesheet  href="/julia-site/css/fonts.css" /> <link rel=stylesheet  href="/julia-site/css/franklin.css" /> <script async defer src="/julia-site/libs/buttons.js"></script> <!-- --> <!-- NOTE: google tracking, off for now <script type="application/javascript"> var doNotTrack = false; if (!doNotTrack) { window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga('create', 'UA-28835595-1', 'auto'); ga('send', 'pageview'); } </script> <script async src='https://www.google-analytics.com/analytics.js'></script> --> <link rel=icon  href="/julia-site/assets/infra/julia.ico"> <title>MLJ Projects – Summer of Code</title> <style> .container ul li p {margin-bottom: 0;} </style> <div class="container py-3 py-lg-0"> <nav class="navbar navbar-expand-lg navbar-light bg-light" id=main-menu > <a class=navbar-brand  href="/julia-site/" id=logo > <img src="/julia-site/assets/infra/logo.svg" height=55  width=85  alt="JuliaLang Logo"/> </a> <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/julia-site/downloads/">Download</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="https://docs.julialang.org">Documentation</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/julia-site/blog/">Blog</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/julia-site/community/">Community</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/julia-site/learning/">Learning</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/julia-site/research/">Research</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/julia-site/jsoc/">JSoC</a> <li class="nav-item donate flex-md-fill text-md-center"> <a class=github-button  href="https://github.com/sponsors/julialang" data-icon=octicon-heart  data-size=large  aria-label="Sponsor @julialang on GitHub">Sponsor</a> </ul> </div> </nav> </div> <br><br> <div class=container > <h1 id=mlj_projects_summer_of_code ><a href="#mlj_projects_summer_of_code">MLJ Projects – Summer of Code</a></h1> <p><a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> is a machine learning framework for Julia aiming to provide a convenient way to use and combine a multitude of tools and models available in the Julia ML/Stats ecosystem. MLJ is released under the MIT licensed and sponsored by the Alan Turing Institute.</p> <p>Project mentors are <a href="https://github.com/ablaom">Anthony Blaom</a>, <a href="https://www.turing.ac.uk/people/programme-directors/sebastian-vollmer">Sebastian Vollmer</a>.</p> <h2 id=fairml ><a href="#fairml">FairML</a></h2> <p>Granting parole to accepting credit applications decision support tools guide human decision making with the aim to improve outcomes. In the latter often without any human in the loop. It is important that these decisions are fair. But what does fair mean? Does it mean we simply don&#39;t feed protective features such as age, gender and ethnicity? No, theses are correlated with location, shoe size etc. Also, what does fair mean does it mean proportionally we want the same false-positive-rate or false-negative rate. But which one? Do we want the proportion that predicted-to-recommit a crime and don&#39;t be equalised or the ones that predicted-to-not-recommit a crime and do be equalised across groups? The first step is auditing the bias - there should be no performance metrics without assessing the bias. The second part is to look at the trade-off between fairness and performance. Letting no one out of jail is fair when comparing different subpopulations but not morally fair. This project looks at</p> <div class=tight-list ><ol> <li><p>auditing</p> <li><p>pre and postprocessing of existing algorithms to improve fairness</p> <li><p>the novel methodology that explores the multiobjective tradeoff between performance and fairness such as to incorporate fairness in training loss.</p> </ol> </div> <p><strong>References</strong>: <div class=tight-list ><ul> <li><p>High-level overview: <a href="">https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb</a></p> <li><p><a href="">https://arxiv.org/abs/1811.05577</a></p> <li><p><a href="">https://github.com/dssg/aequitas</a></p> <li><p><a href="">https://arxiv.org/abs/2001.09233</a></p> </ul> </div> <h2 id=parallel_computing_and_mlj_test ><a href="#parallel_computing_and_mlj_test">Parallel computing and MLJ Test</a></h2> <p><strong>Project idea</strong>: Parallel computing and MLJ Test, benchmark and experiment with MLJ&#39;s parallelization features. MLJ offers Distributed processing and multithreading for: individual model training, model performance evaluation &#40;eg, by cross-validation&#41;, when creating homogenous model ensembles, and when optimising model hyper parameters &#40;model tuning&#41;. Tuning-&gt;&#40;ensembling&#41;-&gt;evaluating-&gt;fitting are operations that are typically composed leading to multiple levels of parallelisation. There is considerable scope for testing and experimentation with parallelisation at the different levels. There is also scope for suggesting and implementing improvements.</p> <h2 id=hyperparameter_tuning_of_iterative_algorithms ><a href="#hyperparameter_tuning_of_iterative_algorithms">Hyperparameter tuning of iterative algorithms</a></h2> <p><strong>Desired skills</strong>: Julia, Gaussian Process. The best performing machine learning pipelines compare and combine multiple atomic models and tune their hyperparameters. One hyperparameter in particular is the number of steps to train a model, as this directly scales the learning cost. For instance, a hyperparameter-tuning-algorithm might decide whether to pause training and start a new model or resume the training of a previously-considered model.</p> <p>This can be misleading. In the early stages of training, Model B looks more promising than Model A even though the latter ultimately performs better. Therefore, we must be careful not to prematurely terminate training for Model A.</p> <p>This project aims at comparing existing methodologies with a few new ideas.</p> <p>References: <div class=tight-list ><ul> <li><p><a href="">https://arxiv.org/pdf/1603.06560</a></p> <li><p><a href="">https://arxiv.org/abs/1406.3896</a></p> <li><p><a href="">https://openreview.net/forum?id&#61;SknC0bW0</a></p> </ul> </div> <h2 id=add_time_series_support ><a href="#add_time_series_support">Add time series support</a></h2> <p>MLJ is so far focused on tabular data. This project is to add support for time series data in a modular, composable way.</p> <p>Time series are everywhere in real-world applications and there has been an increase in interest in time series toolboxes recently &#40;see e.g. <a href="https://github.com/alan-turing-institute/sktime">sktime</a>, <a href="https://github.com/rtavenar/tslearn">tslearn</a>, <a href="https://github.com/uea-machine-learning/tsml/">tsml</a>&#41;.</p> <p>But there are still very few principled time-series libraries out there, so you would be working on something that could be very useful for a large number of people. To find out more, check out this <a href="http://learningsys.org/neurips19/assets/papers/sktime_ml_systems_neurips2019.pdf">paper</a> on sktime.</p> <p>The main goal of the project is to port capability from sktime to MLJ.</p> <h3 id=mentors ><a href="#mentors">Mentors</a></h3> <div class=tight-list ><ul> <li><p><a href="https://www.turing.ac.uk/people/programme-directors/sebastian-vollmer">Sebastian Vollmer</a></p> <li><p><a href="https://github.com/mloning">Markus Löning</a> &#40;sktime developer&#41;</p> </ul> </div> <h3 id=details ><a href="#details">Details</a></h3> <div class=tight-list ><ul> <li><p>Extend existing MLJ data container to handle time series data, </p> <li><p>Implement time series data specific transformers that work on new data container &#40;e.g. Fourier transform, auto-correlation, etc.&#41;,</p> <li><p>Implement time-series classification algorithms &#40;for a good overview of algorithms, see e.g. this <a href="https://arxiv.org/abs/1602.01711">paper</a>; for Python implementations see e.g. <a href="https://github.com/alan-turing-institute/sktime">sktime</a>; for Java implementations, see e.g. <a href="http://www.timeseriesclassification.com/code.php">this code repository</a>&#41;,</p> </ul> </div> <h3 id=where_to_get_started ><a href="#where_to_get_started">Where to get started?</a></h3> <p>A good starting point that combines all of the above implementations may be <a href="https://www.sciencedirect.com/science/article/pii/S0020025513001473">time series forest</a>, a generalisation of random forest in which each tree of the ensemble extracts features from random intervals of the time series before fitting a decision tree on the extracted features, so that each tree is no longer a single estimator but rather a pipeline chaining prior transformations with a final estimator.</p> <h2 id=bring_mlj_to_kaggle ><a href="#bring_mlj_to_kaggle">Bring MLJ to Kaggle&#33;</a></h2> <p><strong>Project idea</strong>: Bring MLJ to Kaggle&#33; See if MLJ and your data science skills are up to the challenge of matching the Kaggle tutorial results of other ML frameworks using Julia. Many Kaggle competitions rely on comparing and combining the predictions of numerous models, and with over 120 models and a maturing selection of meta-modelling tools, MLJ is poised to enter the fray. Help us lure more data scientists to Julia, and help us identity MLJ shortcomings, by developing end-to-end applications of MLJ tools and models to Kaggle tutorials. </div> <br><br> <footer class="container-fluid footer-copy"> <div class=container > <div class=row > <div class="col-md-10 py-2"> <p>We thank <a style="color: #7a95dd" href="https://www.fastly.com">Fastly</a> for their generous infrastructure support. Donations help pay for community resources such as CI, Discourse, workshops, travel, JuliaCon, and other such needs.</p> <p>©2020 JuliaLang.org contributors. The website content uses the <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>; it has been built using <a style="color: #7a95dd" href="https://franklinjl.org">Franklin.jl</a> - a Julia native package for building websites.</p> </div> <div class="col-md-2 py-2"> <a class=github-button  href="https://github.com/sponsors/julialang" data-icon=octicon-heart  data-size=large  aria-label="Sponsor @julialang on GitHub">Sponsor</a> </div> </div> </div> </footer> <script src="/julia-site/libs/jquery/jquery.min.js"></script> <script src="/julia-site/libs/bootstrap/bootstrap.min.js"></script> <script src="/julia-site/libs/platform.js"></script>