<!doctype html>
<html lang="en">
<head>
	<!-- parts for all pages -->
	<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." />
<meta name="description" content="The official website for the Julia Language. Julia is a language that is fast, dynamic, easy to use, and open source. Click here to learn more."/>
<meta property="og:title" content="The Julia Language"/>
<meta property="og:image" content="http://www.julialang.org/images/julia-open-graph.png"/>
<meta property="og:description" content="Official website for the Julia programming language"/>

	<!-- NOTE: specific stylesheets, off for now -->
<link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel="stylesheet">
<link rel="stylesheet" href="/libs/bootstrap/bootstrap.min.css" />
<link rel="stylesheet" href="/css/app.css" />
<link rel="stylesheet" href="/css/fonts.css" />
<link rel="stylesheet" href="/css/franklin.css" />
<script async defer src="/libs/buttons.js"></script>

<!-- -->

<!-- NOTE: google tracking, off for now
<script type="application/javascript">
	var doNotTrack = false;
	if (!doNotTrack) {
		window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
		ga('create', 'UA-28835595-1', 'auto');
		ga('send', 'pageview');
	}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
-->


  <!-- Franklin stylesheets for generated pages -->
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
      
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
  
   <title>NeuralNetDiffEq.jl: A Neural Network solver for ODEs</title>   

  
  <style>
	  .container ul li p {margin-bottom: 0;}
  </style>
  

</head>

<body>

<div class="container py-3 py-lg-0">
  <nav class="navbar navbar-expand-lg navbar-light bg-light" id="main-menu">
    <!-- LOGO -->
    <a class="navbar-brand" href="/" id="logo">
      <img src="/assets/infra/logo.svg" height="55" width="85" alt="JuliaLang Logo"/>
    </a>

    <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

      <!-- MENU: DOWNLOAD | DOCUMENTATION | BLOG ... -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/downloads/">Download</a>
        </li>
        <li class="nav-item flex-md-fill text-md-center">
          <a class="nav-link" href="https://docs.julialang.org">Documentation</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/community/">Community</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/learning/">Learning</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/research/">Research</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/jsoc/">JSoC</a>
        </li>
        <li class="nav-item donate flex-md-fill text-md-center">
          <a class="github-button" href="https://github.com/sponsors/julialang" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @julialang on GitHub">Sponsor</a>
        </li>
      </ul>
    </div>

  </nav>
</div>


<br><br>


<!-- Content appended here -->

<div class="container">
<h1 id="neuralnetdiffeqjl_a_neural_network_solver_for_odes"><a href="#neuralnetdiffeqjl_a_neural_network_solver_for_odes">NeuralNetDiffEq.jl: A Neural Network solver for ODEs</a></h1>
<p>My GSoC 2017 <a href="https://summerofcode.withgoogle.com/projects/#5850956641075200">project</a> was to implement a package for Julia to solve Ordinary Differential Equations using Neural Networks. The purpose of the project was to provide an additional DE solver using Neural Networks which has parallelism in time as the key advantage over other solvers which are iterative in nature. The project was based on research paper of <a href="https://arxiv.org/pdf/physics/9705023.pdf">Lagaris et al. 1997</a> which proposed the function approximation capabilities of neural networks &#40;NNs&#41; for solving differential equations. The project was a mixture of research as well as implementation aspects and still has a few parts left to work upon. I chose to work on this project as I have interest in mathematics and machine learning and it involved concepts of both the fields. The package uses <a href="https://github.com/JuliaDiffEq/DifferentialEquations.jl">DifferentialEquations.jl</a> for the solver interface and <a href="https://github.com/denizyuret/Knet.jl">KNet.jl</a> for NN solver implementation.</p>
<h2 id="how_to_use_neural_network_for_solving_differential_equations"><a href="#how_to_use_neural_network_for_solving_differential_equations">How to use Neural Network for solving Differential Equations?</a></h2>  The concept of this solver is based on the UAT &#40;Universal Approximation Theorem&#41; which says that a NN with at least one hidden layer can approximate any continuous function. The neural network is made to minimize a loss function, defined as the difference between the NN&#39;s derivative and the derivative of the differential equation, which then results in the convergence of our trial solution towards the actual &#40;analytical&#41; solution of the differential equation. To know more about UAT <a href="http://neuralnetworksanddeeplearning.com/chap4.html">click here</a>.</p>
<h2 id="research_aspect_of_the_project_and_the_challenge"><a href="#research_aspect_of_the_project_and_the_challenge">Research aspect of the project and the challenge</a></h2>  The research paper we referred to on the topic is quite old and understanding the examples as well as explanations was quite challenging. Not much research has been done on using NNs for this purpose and thus we were not able to get much help from the research papers related to the topic. The initial task was to read and understand the mathematics behind solving differential equations. Also the computational methods used to solve differential equations on computers are quite different from the ones we use on paper so it took quite some time to get familiar with them. The structure and type of NN to be used so that the solver advantages &#40;parallelism in time&#41; are retained without compromising the performance was a research subdomain as well was a challenge.</p>
<p>After implementing the solver for ODEs &#40;Ordinary Differential Equations&#41; and systems of ODEs, the difficult part was to make the NN converge for the systems of ODEs on longer time domains. As there are a lot of factors involved in neural networks, like hidden layer width, number of hidden neurons, activations, weights etc., I relied on my machine learning background as well as the help from my mentors to experiment with most of the feasible settings of NN hyper-parameters and recording the accuracy of convergence and performance of the solver. Making the NNs converge for systems of ODEs is not as easy as it seems and took up most of the time for experimentation and tuning. Predicting the system of DEs solution with larger domain is still a challenge which needs to be worked upon.</p>
<h2 id="implementation_and_work"><a href="#implementation_and_work">Implementation and Work</a></h2>  The implementation involved integration of mathematical and machine learning aspects to build a neural net solver for ODEs. The <a href="https://github.com/JuliaDiffEq/DiffEqBase.jl">DiffEqBase library</a> is used as a base to extend the algorithm and solver interface while the neural network was developed using the <a href="https://github.com/denizyuret/Knet.jl">Knet.jl library</a>. The work done till now can be seen on the <a href="https://github.com/JuliaDiffEq/NeuralNetDiffEq.jl">NeuralNetDiffEq.jl github repository</a>, primarily in <a href="https://github.com/JuliaDiffEq/NeuralNetDiffEq.jl/tree/SingleNN_Approach">this branch</a>. This work involves implementing a Neural Network solver for ODEs with customized interpolation based on NN prediction.</p>
<h3 id="how_does_it_work"><a href="#how_does_it_work">How does it work?</a></h3>  We construct a trial solution for our differential equation in terms of the NN output which should also satisfy the DE boundary conditions. We define a loss function for the neural net which is the difference between the derivative of the neural net solution with regards to its input and the true derivative defined by the ODE. This is an unusual loss function, in that in includes the gradient of the network itself. It is almost unseen elsewhere in other ML applications This loss function is minimized &#40;by equating the derivative difference to zero&#41; using the NN &#40;closer to 0 better convergence&#41; with the trial solution substituted in it in place of the original function &#40;or the solution to the DE&#41;. The neural network tunes its weights using the Adam optimization algorithm on the backpropagated gradients from that loss function.</p>
<p>For parallel implementation in time we use KnetArray &#40;the array type used in <a href="https://github.com/denizyuret/Knet.jl">KNet.jl</a>&#41; which uses CPU by default but GPU usage is also supported for parallelism but requires CUDNN driver installed to access GPU hardware.</p>
<h2 id="examples"><a href="#examples">Examples</a></h2>  Below you can find a few examples on how to use the package I&#39;ve been working on. Following are the initial imports required for the package to work. <pre><code class="language-julia">using NeuralNetDiffEq
using Plots; plotly()
using DiffEqBase, ParameterizedFunctions
using DiffEqProblemLibrary, DiffEqDevTools
using Knet</code></pre> <h3 id="ode_examples"><a href="#ode_examples">ODE Examples</a></h3>  <h4 id="example_1"><a href="#example_1">Example 1</a></h4>
<pre><code class="language-julia">linear = (t,u) -> (1.01*u)
(f::typeof(linear))(::Type{Val{:analytic}},t,u0) = u0*exp(1.01*t)
prob = ODEProblem(linear,1/2,(0.0,1.0))
sol = solve(prob,nnode(10),dt=1/10,iterations=10)
plot(sol,plot_analytic=true)</code></pre>
<p><img src="/assets/images/blog/2017-09-04-gsoc-NeuralNetDiffEq/plot_ode1.png" alt="Plot_ode1" /></p>
<pre><code class="language-julia">sol(0.232)

1-element Array{Any,1}:
0.625818</code></pre>
<h4 id="example_2"><a href="#example_2">Example 2</a></h4>
<pre><code class="language-julia">f = (t,u) -> (t^3 + 2*t + (t^2)*((1+3*(t^2))/(1+t+(t^3))) - u*(t + ((1+3*(t^2))/(1+t+t^3))))
(::typeof(f))(::Type{Val{:analytic}},t,u0) =  u0*exp(-(t^2)/2)/(1+t+t^3) + t^2
prob2 = ODEProblem(f,1.0,(0.0,1.0))
sol2 = solve(prob2,nnode(10),dt=0.1,iterations=200)
 plot(sol,plot_analytic=true)</code></pre> <img src="/assets/images/blog/2017-09-04-gsoc-NeuralNetDiffEq/plot_ode2.png" alt="Plot_ode2" /></p>
<pre><code class="language-julia">sol(0.47)

1-element Array{Any,1}:
0.803109</code></pre>
<h4 id="example_3"><a href="#example_3">Example 3</a></h4>
<pre><code class="language-julia">f2 = (t,u) -> (-u/5 + exp(-t/5).*cos(t))
(::typeof(f2))(::Type{Val{:analytic}},t,u0) =  exp(-t/5)*(u0 + sin(t))
prob3 = ODEProblem(f2,Float32(0.0),(Float32(0.0),Float32(2.0)))
sol3 = solve(prob3,nnode(10),dt=0.2,iterations=1000)
 plot(sol,plot_analytic=true)</code></pre>
<p><img src="/assets/images/blog/2017-09-04-gsoc-NeuralNetDiffEq/plot_ode3.png" alt="Plot_ode3" /></p>
<pre><code class="language-julia">sol3([0.721])

  1-element Array{Any,1}:
  Any[0.574705]</code></pre>
<h3 id="system_of_odes_examples"><a href="#system_of_odes_examples">System of ODEs Examples</a></h3>
<h4 id="example_1_ode_2d_linear"><a href="#example_1_ode_2d_linear">Example 1  ODE 2D Linear</a></h4>
<pre><code class="language-julia">f_2dlinear = (t,u) -> begin
    du = Array{Any}(length(u))
    for i in 1:length(u)
    du[i] = 1.01*u[i]
  end
    return du
end
(f::typeof(f_2dlinear))(::Type{Val{:analytic}},t,u0) = u0*exp.(1.01*t)
prob_ode_2Dlinear = ODEProblem(f_2dlinear,rand(4,1),(0.0,1.0))
sol1 = solve(prob_ode_2Dlinear,nnode([10,50]),dt=0.1,iterations=100)

(:iteration,100,:loss,0.004670103680503722)
16.494870 seconds (90.08 M allocations: 6.204 GB, 5.82% gc time)</code></pre>
<pre><code class="language-julia">plot(sol1,plot_analytic=true)</code></pre>
<p><img src="/assets/images/blog/2017-09-04-gsoc-NeuralNetDiffEq/plot_ode1.png" alt="Plot_sode1" /></p>
<h4 id="example_2_lotka_volterra"><a href="#example_2_lotka_volterra">Example 2 Lotka Volterra</a></h4>
<pre><code class="language-julia">function lotka_volterra(t,u)
  du1 = 1.5 .* u[1] - 1.0 .* u[1].*u[2]
  du2 = -3 .* u[2] + u[1].*u[2]
  [du1,du2]
end

lotka_volterra (generic function with 1 method)</code></pre>
<pre><code class="language-julia">prob_ode_lotkavoltera = ODEProblem(lotka_volterra,Float32[1.0,1.0],(Float32(0.0),Float32(1.0)))
sol2 = solve(prob_ode_lotkavoltera,nnode([10,50]),dt=0.2,iterations=1000)

    (:iteration,100,:loss,0.020173132003438572)
    (:iteration,200,:loss,0.005130137452114811)
    (:iteration,300,:loss,0.004812458584875589)
    (:iteration,400,:loss,0.010083624565714974)
    (:iteration,500,:loss,0.0025328170079611887)
    (:iteration,600,:loss,0.007685579218433846)
    (:iteration,700,:loss,0.005065291031504465)
    (:iteration,800,:loss,0.005326863832044214)
    (:iteration,900,:loss,0.00030436474139241827)
    (:iteration,1000,:loss,0.0034853904995959094)
     22.126081 seconds (99.65 M allocations: 5.923 GB, 5.21% gc time)</code></pre>
<pre><code class="language-julia">plot(sol2)</code></pre>
<p><img src="/assets/images/blog/2017-09-04-gsoc-NeuralNetDiffEq/plot_sode2.png" alt="Plot_sode2" /></p>
<p>To show that the solver with current settings and hyper-parameters does not work for a larger domain &#40;Eg 0-10&#41; <code>lotka_volterra</code> here is a graph:</p>
<pre><code class="language-julia">prob_ode_lotkavoltera = ODEProblem(lotka_volterra,Float32[1.0,1.0],(Float32(0.0),Float32(5.0)))
sol3 = solve(prob_ode_lotkavoltera,nnode([10,50]),dt=0.2,iterations=1000)
plot(sol3)</code></pre> <img src="/assets/images/blog/2017-09-04-gsoc-NeuralNetDiffEq/plot_sode3.png" alt="Plot_sode3" /></p>
<p>However, the <a href="http://app.juliadiffeq.org/ode;settings&#61;eyJkaWZmRXFUZXh0IjoiZHggPSBhKnggLSBiKngqeVxuZHkgPSAtYyp5ICsgZCp4KnkiLCJwYXJhbWV0ZXJzIjoiYT0xLjUsIGI9MSwgYz0zLCBkPTEiLCJ0aW1lU3BhbiI6WzAsMTBdLCJpbml0aWFsQ29uZGl0aW9ucyI6IjEuMCwgMS4wIiwic29sdmVyIjoiVHNpdDUiLCJ2YXJzIjoiWzp4LCA6eV0iLCJ0aXRsZSI6IlRoZSBMb3RrYS1Wb2x0ZXJyYSBFcXVhdGlvbnM6IE1vZGVsIG9mIFJhYmJpdHMgYW5kIFdvbHZlcyJ9">true solution should be oscillatory, indicating that the NN did not properly converge</a>. To see more examples and experiment results you can check out my Jupyter notebooks <a href="http://nbviewer.jupyter.org/gist/akaysh/43c9db281b0bd3224114084c44263c13">here</a>.</p>
<h2 id="future_work"><a href="#future_work">Future Work</a></h2>  More of research on how to optimize the NN for speed and better convergence is required. For systems of ODEs with larger domains the current Neural Network fails to converge. An optimization algorithm can be used for one time NN hyperparameter optimization so that it can work better for systems of ODEs. We tried many approaches like biasing the cost function to prioritize earlier timepoints but this failed as well. Similar problems were found in an <a href="https://github.com/JuliaDiffEq/TensorFlowDiffEq.jl">alternative implementation using TensorFlow &#40;TensorFlowDiffEq.jl&#41;</a>, which suggests this may just be a problem with the solving method.</p>
<h2 id="acknowledgements"><a href="#acknowledgements">Acknowledgements</a></h2>
<p>I would really want to thank my GSoC mentors Chris Rackauckas and Lyndon White for the help they provided in understanding mathematical as well as coding parts of the project. Also I would like to thank the Julia community in general for giving me opportunity to contribute and for sponsoring my JuliaCon 2017 trip which was awesome.

</div>
<br><br>

<!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    <footer class="container-fluid footer-copy">
  <div class="container">
    <div class="row">
      <div class="col-md-10 py-2">
        <p>We thank <a style="color: #7a95dd" href="https://www.fastly.com">Fastly</a> for their generous infrastructure support.
        Donations help pay for community resources such as CI, Discourse, workshops, travel, JuliaCon, and other such needs.</p>
        <p>Â©2020 JuliaLang.org contributors. The website content uses the <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>; it has been built using <a style="color: #7a95dd" href="https://franklinjl.org">Franklin.jl</a> - a Julia native package for building websites.</p>
      </div>
      <div class="col-md-2 py-2">
        <a class="github-button" href="https://github.com/sponsors/julialang" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @julialang on GitHub">Sponsor</a>
      </div>
    </div>
  </div>
</footer>

<script src="/libs/jquery/jquery.min.js"></script>
<script src="/libs/bootstrap/bootstrap.min.js"></script>
<script src="/libs/platform.js"></script>
<!-- <script src="/libs/highlight/highlight.pack.js"></script> -->
<!-- <script>hljs.initHighlightingOnLoad();</script> -->


  </body>
</html>
